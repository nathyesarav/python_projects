{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f2c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"test.txt\", encoding=\"UTF-8\")\n",
    "data = file.read()\n",
    "textos = data.split('\\nQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4f534b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q1 Explain the difference between supervised and unsupervised machine  learning? \\nIn supervised machine learning algorithms, we have to provide labeled data, for example,  prediction of stock market prices, whereas in unsupervised we need not have labeled data, for  example, classification of emails into spam and non-spam.  ',\n",
       " '2 What are the parametric models? Give an example.  \\nParametric models are those with a finite number of parameters. To predict new data, you only  need to know the parameters of the model. Examples include linear regression, logistic  regression, and linear SVMs.  \\nNon-parametric models are those with an unbounded number of parameters, allowing for more  flexibility. To predict new data, you need to know the parameters of the model and the state of  the data that has been observed. Examples include decision trees, k-nearest neighbors, and  topic models using latent Dirichlet analysis.  ',\n",
       " '3 What is the difference between classification and regression? Classification is used to produce discrete results, classification is used to clas sify data into some specific categories. For example, classifying emails into spam and non-spam categories.  Whereas, We use regression analysis when we are dealing with continuous data, for example  predicting stock prices at a certain point in time.  ',\n",
       " '4 What Is Overfitting, and How Can You Avoid It?  \\nOverfitting is a situation that occurs when a model learns the training set too well, taking up  random fluctuations in the training data as concepts. These impact the model’s ability to  generalize and don’t apply to new data.  \\nWhen a model is given the training data, it shows 100 percent accuracy—technically a slight  loss. But, when we use the test data, there may be an error and low efficiency. This condition is  known as overfitting.  \\nThere are multiple ways of avoiding overfitting, such as:  \\n● Regularization. It involves a cost term for the features involved with the objective function  ● Making a simple model. With lesser variables and parameters, the variance can be  reduced  \\n● Cross-validation methods like k-folds can also be used  \\n● If some model parameters are likely to cause overfitting, techniques for regularization  like LASSO can be used that penalize these parameters  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '5 What is meant by ‘Training set’ and ‘Test Set’?  \\nWe split the given data set into two different sections namely,’Training set’ and ‘Test Set’.  ‘Training set’ is the portion of the dataset used to train the model.  \\n‘Testing set’ is the portion of the dataset used to test the trained model.  ',\n",
       " '6 How Do You Handle Missing or Corrupted Data in a Dataset?  One of the easiest ways to handle missing or corrupted data is to drop those rows or columns or  replace them entirely with some other value.  \\nThere are two useful methods in Pandas:  \\n● IsNull() and dropna() will help to find the columns/rows with missing data and drop them  ● Fillna() will replace the wrong values with a placeholder value  ',\n",
       " '7 Explain Ensemble learning. \\nIn ensemble learning, many base models like classifiers and regressors are generated and  combined together so that they give better results. It is used when we build component  classifiers that are accurate and independent. There are sequential as well as parallel ensemble  methods.  ',\n",
       " \"8 Explain the Bias-Variance Tradeoff.  \\nPredictive models have a tradeoff between bias (how well the model fits the data) and variance  (how much the model changes based on changes in the inputs).  \\nSimpler models are stable (low variance) but they don't get close to the truth (high bias).  More complex models are more prone to overfitting (high variance) but they are expressive  enough to get close to the truth (low bias). The best model for a given problem usually lies  somewhere in the middle.  \",\n",
       " \"9 What is the difference between stochastic gradient descent (SGD) and  gradient descent (GD)?  \\nBoth algorithms are methods for finding a set of parameters that minimize a loss function by  evaluating parameters against data and then making adjustments.  \\nIn standard gradient descent, you'll evaluate all training samples for each set of parameters.  This is akin to taking big, slow steps toward the solution.  \\nIn stochastic gradient descent, you'll evaluate only 1 training sample for the set of parameters  before updating them. This is akin to taking small, quick steps toward the solution.  \",\n",
       " '10 How Can You Choose a Classifier Based on a Training Set Data Size?  When the training set is small, a model that has a right bias and low variance seems to work  better because they are less likely to overfit.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/\\nFor example, Naive Bayes works best when the training set is large. Models with low bias and  high variance tend to perform better as they work fine with complex relationships.  ',\n",
       " \"11 What are 3 data preprocessing techniques to handle outliers?  1. Winsorize (cap at threshold).  \\n2. Transform to reduce skew (using Box-Cox or similar).  \\n3. Remove outliers if you're certain they are anomalies or measurement errors.  \",\n",
       " \"12 How much data should you allocate for your training, validation, and test  sets?  \\nYou have to find a balance, and there's no right answer for every problem.  If your test set is too small, you'll have an unreliable estimation of model performance  (performance statistic will have high variance). If your training set is too small, your actual model parameters will have a high variance.  \\nA good rule of thumb is to use an 80/20 train/test split. Then, your train set can be further split  into train/validation or into partitions for cross-validation.  \",\n",
       " '13 What Is a False Positive and False Negative and How Are They Significant?  False positives are those cases which wrongly get classified as True but are False.  False negatives are those cases which wrongly get classified as False but are True.  In the term ‘False Positive,’ the word ‘Positive’ refers to the ‘Yes’ row of the predi cted value in the confusion matrix. The complete term indicates that the system has predicted it as a positi ve, but the actual value is negative.  ',\n",
       " '14 Explain the difference between L1 and L2 regularization. \\nL2 regularization tends to spread error among all the terms, while L1 is more binary/sparse, with  many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a  Laplacean prior to the terms, while L2 corresponds to a Gaussian prior.  ',\n",
       " '15 What’s a Fourier transform?  \\nA Fourier transform is a generic method to decompose generic functions into a superposition of  symmetric functions. Or as this more intuitive tutorial puts it, given a smoothie, it ’s how we find the recipe. The Fourier transform finds the set of cycle speeds, amplitudes, and phases to  match any time signal. A Fourier transform converts a signal from time to frequency domain —  it’s a very common way to extract features from audio signals or other time series such as  sensor data.  ',\n",
       " '16 What is deep learning, and how does it contrast with other machine learning  algorithms?  \\nDeep learning is a subset of machine learning that is concerned with neural networks: how to  use backpropagation and certain principles from neuroscience to more accurately model large  sets of unlabelled or semi-structured data. In that sense, deep learning represents an  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/\\nunsupervised learning algorithm that learns representations of data through the use of neural  nets.  ',\n",
       " '17 What’s the difference between a generative and discriminative model?  A generative model will learn categories of data while a discriminative model will simply learn  the distinction between different categories of data. Discriminative models will generally  outperform generative models on classification tasks.  ',\n",
       " '18 What Are the Applications of Supervised Machine Learning in Modern  Businesses?  \\nApplications of supervised machine learning include:  \\n● Email Spam Detection \\n Here we train the model using historical data that consists of emails categorized as spam or not spam. This labeled information is fed as input to the model.  ● Healthcare Diagnosis \\n By providing images regarding a disease, a model can be trained to detect if a person is suffering from the disease or not.  \\n● Sentiment Analysis \\n This refers to the process of using algorithms to mine documents and determine whether they’re positive, neutral, or negative in sentiment.  \\n● Fraud Detection \\n Training the model to identify suspicious patterns, we can detect instances of possible fraud.  ',\n",
       " '19 What Is Semi-supervised Machine Learning? \\nSupervised learning uses data that is completely labeled, whereas unsupervised learning uses  no training data.  \\nIn the case of semi-supervised learning, the training data contains a small amount of labeled  data and a large amount of unlabeled data.  ',\n",
       " '20. What Are Unsupervised Machine Learning Techniques?  \\nThere are two techniques used in unsupervised learning: clustering and association.  Clustering  \\n● Clustering problems involve data to be divided into subsets. These subsets, also called  clusters, contain data that are similar to each other. Different clusters reveal different  details about the objects, unlike classification or regression.  \\nAssociation  \\n● In an association problem, we identify patterns of associations between different  variables or items.  \\n● For example, an eCommerce website can suggest other items for you to buy, based on  the prior purchases that you have made, spending habits, items in your wishlist, other  customers’ purchase habits, and so on.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '21 What Is ‘naive’ in the Naive Bayes Classifier?  \\nThe classifier is called ‘naive’ because it makes assumptions that may or may not turn out to be  correct.  \\nThe algorithm assumes that the presence of one feature of a class is not related to the presence of any other feature (absolute independence of features), given the class variable.  For instance, a fruit may be considered to be a cherry if it is red in color and round in shape,  regardless of other features. This assumption may or may not be right (as an apple also  matches the description).  ',\n",
       " '22 Explain Latent Dirichlet Allocation (LDA).  \\nLatent Dirichlet Allocation (LDA) is a common method of topic modeling, or classifying  documents by subject matter.  \\nLDA is a generative model that represents documents as a mixture of topics that each have  their own probability distribution of possible words.  \\nThe \"Dirichlet\" distribution is simply a distribution of distributions. In LDA, document s are distributions of topics that are distributions of words.  ',\n",
       " '23 Explain Principle Component Analysis (PCA).  \\nPCA is a method for transforming features in a dataset by combining them into uncorrelated  linear combinations.  \\nThese new features, or principal components, sequentially maximize the variance represented  (i.e. the first principal component has the most variance, the second principal component has  the second most, and so on).  \\nAs a result, PCA is useful for dimensionality reduction because you can set an arbitrary  variance cutoff.  ',\n",
       " '24 What’s the F1 score? How would you use it?  \\nThe F1 score is a measure of a model’s performance. It is a weighted average of the precision  and recall of a model, with results tending to 1 being the best, and those tending to 0 being the  worst. You would use it in classification tests where true negatives don’t matter much.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/ ',\n",
       " '25 When should you use classification over regression?  \\nClassification produces discrete values and dataset to strict categories, while regression giv es you continuous results that allow you to better distinguish differences between individual points.  You would use classification over regression if you wanted your results to reflect the  belongingness of data points in your dataset to certain explicit categories (ex: If you wanted to  know whether a name was male or female rather than just how correlated they were with male  and female names.)  ',\n",
       " '26 How do you ensure you’re not overfitting with a model?  \\nThis is a simple restatement of a fundamental problem in machine learning: the possibility of  overfitting training data and carrying the noise of that data through to the test set, thereby  providing inaccurate generalizations.  \\nThere are three main methods to avoid overfitting:  \\n1- Keep the model simpler: reduce variance by taking into account fewer variables and  parameters, thereby removing some of the noise in the training data.  \\n2- Use cross-validation techniques such as k-folds cross-validation.  \\n3- Use regularization techniques such as LASSO that penalize certain model parameters if  they’re likely to cause overfitting.  ',\n",
       " '27 How Will You Know Which Machine Learning Algorithm to Choose for Your  Classification Problem?  \\nWhile there is no fixed rule to choose an algorithm for a classification problem, you can f ollow these guidelines:  \\n● If accuracy is a concern, test different algorithms and cross-validate them  ● If the training dataset is small, use models that have low variance and high bias  ● If the training dataset is large, use models that have high variance and little bias  ',\n",
       " '28 How Do You Design an Email Spam Filter?  \\nBuilding a spam filter involves the following process:  \\n● The email spam filter will be fed with thousands of emails  \\n● Each of these emails already has a label: ‘spam’ or ‘not spam.’  \\n● The supervised machine learning algorithm will then determine which type of emails are  being marked as spam based on spam words like the lottery, free offer, no money, full  refund, etc.  \\n● The next time an email is about to hit your inbox, the spam filter will use statistical  analysis and algorithms like Decision Trees and SVM to determine how likely the email  is spam  \\n● If the likelihood is high, it will label it as spam, and the email won’t hit your inbox  Steve Nouri https://www.linkedin.com/in/stevenouri/ \\n● Based on the accuracy of each model, we will use the algorithm with the highest  accuracy after testing all the models  ',\n",
       " '29 What evaluation approaches would you work to gauge the effectiveness of a  machine learning model?  \\nYou would first split the dataset into training and test sets, or perhaps use cross-validation  techniques to further segment the dataset into composite sets of training and test sets within the  data. You should then implement a choice selection of performance metrics: here is a fairly  comprehensive list. You could use measures such as the F1 score, the accuracy, and the  confusion matrix. What’s important here is to demonstrate that you understand the nuances of  how a model is measured and how to choose the right performance measures for the right  situations.  ',\n",
       " '30 How would you implement a recommendation system for our company’s  users?  \\nA lot of machine learning interview questions of this type will involve the implementation of  machine learning models to a company’s problems. You’ll have to research the company and its  industry in-depth, especially the revenue drivers the company has, and the types of users the  company takes on in the context of the industry it’s in.  ',\n",
       " '31 Explain bagging.  \\nBagging, or Bootstrap Aggregating, is an ensemble method in which the dataset is first divided  into multiple subsets through resampling.  \\nThen, each subset is used to train a model, and the final predictions are made through voting or  averaging the component models.  \\nBagging is performed in parallel.  ',\n",
       " \"32 What is the ROC Curve and what is AUC (a.k.a. AUROC)?  The ROC (receiver operating characteristic) the performance plot for binary classifiers of T rue Positive Rate (y-axis) vs. False Positive Rate (x \\naxis).  \\nAUC is the area under the ROC curve, and it's a common performance metric for evaluating  binary classification models.  \\nIt's equivalent to the expected probability that a uniformly drawn random positive is ranked  before a uniformly drawn random negative.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/\",\n",
       " \"33 Why is Area Under ROC Curve (AUROC) better than raw accuracy as an  out-of-sample evaluation metric?  \\nAUROC is robust to class imbalance, unlike raw accuracy.  \\nFor example, if you want to detect a type of cancer that's prevalent in only 1% of the population,  you can build a model that achieves 99% accuracy by simply classifying everyone has  cancer-free.  \",\n",
       " '34 What are the advantages and disadvantages of neural networks?  Advantages: Neural networks (specifically deep NNs) have led to performance breakthroughs  for unstructured datasets such as images, audio, and video. Their incredible flexibility allows  them to learn patterns that no other ML algorithm can learn.  \\nDisadvantages: However, they require a large amount of training data to converge. It\\'s also  difficult to pick the right architecture, and the internal \"hidden\" layers are incomprehensible.  ',\n",
       " '35 Define Precision and Recall.  \\nPrecision  \\n● Precision is the ratio of several events you can correctly recall to the total number of  events you recall (mix of correct and wrong recalls).  \\n● Precision = (True Positive) / (True Positive + False Positive)  \\nRecall  \\n● A recall is the ratio of a number of events you can recall the number of total events.  ● Recall = (True Positive) / (True Positive + False Negative)  ',\n",
       " '36 What Is Decision Tree Classification?  \\nA decision tree builds classification (or regression) models as a tree structure, with datas ets broken up into ever-smaller subsets while developing the decision tree, literally in a tree-li ke way with branches and nodes. Decision trees can handle both categorical and numerical data.  ',\n",
       " '37 What Is Pruning in Decision Trees, and How Is It Done?  \\nPruning is a technique in machine learning that reduces the size of decision trees. It reduces the  complexity of the final classifier, and hence improves predictive accuracy by the reduction of  overfitting.  \\nPruning can occur in:  \\n● Top-down fashion. It will traverse nodes and trim subtrees starting at the root  ● Bottom-up fashion. It will begin at the leaf nodes  \\nThere is a popular pruning algorithm called reduced error pruning, in which:  ● Starting at the leaves, each node is replaced with its most popular class  ● If the prediction accuracy is not affected, the change is kept  \\n● There is an advantage of simplicity and speed  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '38 What Is a Recommendation System?  \\nAnyone who has used Spotify or shopped at Amazon will recognize a recommendation system:  It’s an information filtering system that predicts what a user might want to hear or see based on  choice patterns provided by the user.  ',\n",
       " '39 What Is Kernel SVM?  \\nKernel SVM is the abbreviated version of the kernel support vector machine. Kernel methods  are a class of algorithms for pattern analysis, and the most common one is the kernel SVM.  ',\n",
       " '40 What Are Some Methods of Reducing Dimensionality?  \\nYou can reduce dimensionality by combining features with feature engineering, removing  collinear features, or using algorithmic dimensionality reduction.  \\nNow that you have gone through these machine learning interview questions, you must have  got an idea of your strengths and weaknesses in this domain.  ',\n",
       " '41 What Are the Three Stages of Building a Model in Machine Learning?  The three stages of building a machine learning model are:  \\n● Model Building Choose a suitable algorithm for the model and train it according to the requirement  \\n● Model Testing Check the accuracy of the model through the test data  ● Applying the Mode Make the required changes after testing and use the final model for  real-time projects. Here, it’s important to remember that once in a while, the model  needs to be checked to make sure it’s working correctly. It should be modified to make  sure that it is up-to-date.  ',\n",
       " '42 How is KNN different from k-means clustering?  \\nK-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an  unsupervised clustering algorithm. While the mechanisms may seem similar at first, what thi s really means is that in order for K-Nearest Neighbors to work, you need labeled data you want  to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires  only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and  gradually learn how to cluster them into groups by computing the mean of the distance between  different points.  ',\n",
       " '43 Mention the difference between Data Mining and Machine learning?  Machine learning relates to the study, design, and development of the algorithms that give  computers the capability to learn without being explicitly programmed. While data mining can  be defined as the process in which the unstructured data tries to extract knowledge or unknown  interesting patterns. During this processing machine, learning algorithms are used.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '44 What are the different Algorithm techniques in Machine Learning?  The different types of techniques in Machine Learning are  \\n● Supervised Learning  \\n● Unsupervised Learning  \\n● Semi-supervised Learning  \\n● Reinforcement Learning  \\n● Transduction  \\n● Learning to Learn  ',\n",
       " '45 You are given a data set. The data set has missing values that spread along 1  standard deviation from the median. What percentage of data would remain  unaffected? Why?  \\nThis question has enough hints for you to start thinking! Since the data is spread across the  median, let’s assume it’s a normal distribution. We know, in a normal distribution, ~68% of the  data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data  unaffected. Therefore, ~32% of the data would remain unaffected by missing values.  ',\n",
       " '46 What are PCA, KPCA, and ICA used for?  \\nPCA (Principal Components Analysis), KPCA ( Kernel-based Principal Component Analysis)  and ICA ( Independent Component Analysis) are important feature extraction techniques used  for dimensionality reduction.  ',\n",
       " '47 What are support vector machines?  \\nSupport vector machines are supervised learning algorithms used for classification and  regression analysis.  ',\n",
       " '48 What is batch statistical learning?  \\nStatistical learning techniques allow learning a function or predictor from a set of observed data  that can make predictions about unseen or future data. These techniques provide guarantees  on the performance of the learned predictor on the future unseen data based on a statistical  assumption on the data generating process.  ',\n",
       " '49 What is the bias-variance decomposition of classification error in the  ensemble method?  \\nThe expected error of a learning algorithm can be decomposed into bias and variance. A bias  term measures how closely the average classifier produced by the learning algorithm matches  the target function. The variance term measures how much the learning algorithm’s prediction  fluctuates for different training sets.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '50 When is Ridge regression favorable over Lasso regression?  You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in the presence of few  variables with medium / large sized effect, use lasso regression. In presence of many variables  with small/medium-sized effects, use ridge regression.  \\nConceptually, we can say, lasso regression (L1) does both variable selection and parameter  shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all  the coefficients in the model. In the presence of correlated variables, ridge regression might be  the preferred choice. Also, ridge regression works best in situations where the least square  estimates have higher variance. Therefore, it depends on our model objective.  ',\n",
       " '51 You’ve built a random forest model with 10000 trees. You got delighted after  getting training error as 0.00. But, the validation error is 34.23. What is going on?  Haven’t you trained your model perfectly?  \\nThe model has overfitted. Training error 0.00 means the classifier has mimicked the training  data patterns to an extent, that they are not available in the unseen data. Hence, when this  classifier was run on an unseen sample, it couldn’t find those patterns and returned predictions  with higher error. In a random forest, it happens when we use a larger number of trees than  necessary. Hence, to avoid this situation, we should tune the number of trees using  cross-validation.  ',\n",
       " '50 What is a convex hull?  \\nIn the case of linearly separable data, the convex hull represents the outer boundaries of the  two groups of data points. Once the convex hull is created, we get maximum margin hyperplane  (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to  create the greatest separation between two groups.  ',\n",
       " '51 What do you understand by Type I vs Type II error?  \\nType I error is committed when the null hypothesis is true and we reject it, also known as a  ‘False Positive’. Type II error is committed when the null hypothesis is false and we accept it, also known as ‘False Negative’.  \\nIn the context of the confusion matrix, we can say Type I error occurs when we classify a value  as positive (1) when it is actually negative (0). Type II error occurs when we classify a value as negative (0) when it is actually positive(1).  ',\n",
       " '52. In k-means or kNN, we use euclidean distance to calculate the distance  between nearest neighbors. Why not manhattan distance?  \\nWe don’t use manhattan distance because it calculates distance horizontally or vertically only. I t has dimension restrictions. On the other hand, the euclidean metric can be used in any space to  calculate distance. Since the data points can be present in any dimension, euclidean distance is  a more viable option.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/\\nExample: Think of a chessboard, the movement made by a bishop or a rook is calculated by  manhattan distance because of their respective vertical & horizontal movements.  ',\n",
       " '53 Do you suggest that treating a categorical variable as a continuous variable  would result in a better predictive model?  \\nFor better predictions, the categorical variable can be considered as a continuous variable only  when the variable is ordinal in nature.  ',\n",
       " '54 OLS is to linear regression. The maximum likelihood is logistic regression.  Explain the statement.  \\nOLS and Maximum likelihood are the methods used by the respective regression methods to  approximate the unknown parameter (coefficient) value. In simple words,  Ordinary least square(OLS) is a method used in linear regression which approximates the  parameters resulting in minimum distance between actual and predicted values. Maximum  Likelihood helps in choosing the values of parameters which maximizes the likelihood that the  parameters are most likely to produce observed data.  ',\n",
       " '55 When does regularization becomes necessary in Machine Learning?  Regularization becomes necessary when the model begins to overfit/underfit. This technique  introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce the cost term. This helps to  reduce model complexity so that the model can become better at predicting (generalizing).  ',\n",
       " '56 What is Linear Regression?  \\nLinear Regression is a supervised Machine Learning algorithm. It is used to find the linear  relationship between the dependent and the independent variables for predictive analysis. ',\n",
       " '57 What is the Variance Inflation Factor?  \\nVariance Inflation Factor (VIF) is the estimate of the volume of multicollinearity i n a collection of many regression variables.  \\nVIF = Variance of the model / Variance of the model with a single independent variable  We have to calculate this ratio for every independent variable. If VIF is high, then it shows the  high collinearity of the independent variables.  ',\n",
       " '58 We know that one hot encoding increases the dimensionality of a dataset,  but label encoding doesn’t. How?  \\nWhen we use one-hot encoding, there is an increase in the dimensionality of a dataset. The  reason for the increase in dimensionality is that, for every class in the categorical variables , it forms a different variable.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '59 What is a Decision Tree?  \\nA decision tree is used to explain the sequence of actions that must be performed to get the  desired output. It is a hierarchical diagram that shows the actions.  ',\n",
       " '60 What is the Binarizing of data? How to Binarize?  \\nIn most of the Machine Learning Interviews, apart from theoretical questions, interviewers focus  on the implementation part. So, this ML Interview Questions focused on the implementation of  the theoretical concepts.  \\nConverting data into binary values on the basis of threshold values is known as the binarizing of  data. The values that are less than the threshold are set to 0 and the values that are greater  than the threshold are set to 1. This process is useful when we have to perform feature  engineering, and we can also use it for adding unique features.  ',\n",
       " '61 What is cross-validation?  \\nCross-validation is essentially a technique used to assess how well a model performs on a new  independent dataset. The simplest example of cross-validation is when you split your data into  two groups: training data and testing data, where you use the training data to build the model  and the testing data to test the model.  ',\n",
       " '62 When would you use random forests Vs SVM and why?  \\nThere are a couple of reasons why a random forest is a better choice of the model than a  support vector machine:  \\n● Random forests allow you to determine the feature importance. SVM’s can’t do this.  ● Random forests are much quicker and simpler to build than an SVM.  ● For multi-class classification problems, SVMs require a one-vs-rest method, which i s less scalable and more memory intensive.  ',\n",
       " '63 What are the drawbacks of a linear model?  \\nThere are a couple of drawbacks of a linear model:  \\n● A linear model holds some strong assumptions that may not be true in the application. It  assumes a linear relationship, multivariate normality, no or little multicollinearity , no auto-correlation, and homoscedasticity  \\n● A linear model can’t be used for discrete or binary outcomes.  \\n● You can’t vary the model flexibility of a linear model.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '64 Do you think 50 small decision trees are better than a large one? Why?  \\nAnother way of asking this question is “Is a random forest a better model than a decision tree?”  And the answer is yes because a random forest is an ensemble method that takes many weak  decision trees to make a strong learner. Random forests are more accurate, more robust, and  less prone to overfitting.  ',\n",
       " '65 What is a kernel? Explain the kernel trick  \\nA kernel is a way of computing the dot product of two vectors   x and   y in some (possibly very high dimensional) feature space, which is why kernel functions are sometimes called  “generalized dot product”  \\nThe kernel trick is a method of using a linear classifier to solve a non-linear problem by  transforming linearly inseparable data to linearly separable ones in a higher dimension.  ',\n",
       " '66 State the differences between causality and correlation?  \\nCausality applies to situations where one action, say X, causes an outcome, say Y, whereas  Correlation is just relating one action (X) to another action(Y) but X does not necessarily cause  Y.  ',\n",
       " '67 What is the exploding gradient problem while using the backpropagation  technique?  \\nWhen large error gradients accumulate and result in large changes in the neural network  weights during training, it is called the exploding gradient problem. The values of weights can  become so large as to overflow and result in NaN values. This makes the model unstable and  the learning of the model to stall just like the vanishing gradient problem.  ',\n",
       " '68 What do you mean by Associative Rule Mining (ARM)?  \\nAssociative Rule Mining is one of the techniques to discover patterns in data like features  (dimensions) which occur together and features (dimensions) which are correlated.  ',\n",
       " '69 What is Marginalisation? Explain the process.  \\nMarginalizationarginalisation is summing the probability of a random variable X given the joint  probability distribution of X with other variables. It is an application of the law of total probability.  ',\n",
       " '70 Why is the rotation of components so important in Principle Component  Analysis(PCA)?  \\nRotation in PCA is very important as it maximizes the separation within the variance obtained by  all the components because of which interpretation of components would become easier. If the  components are not rotated, then we need extended components to describe the variance of  the components.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '71 What is the difference between regularization and normalisation?  Normalisation adjusts the data; regularisation adjusts the prediction function. If your data is on  very different scales (especially low to high), you would want to normalise the data. Alter each  column to have compatible basic statistics. This can be helpful to make sure there is no l oss of accuracy. One of the goals of model training is to identify the signal and ignore the noise if the  model is given free rein to minimize error, there is a possibility of suffering from ov erfitting. Regularization imposes some control on this by providing simpler fitting functions over complex  ones.  ',\n",
       " '72 When does the linear regression line stop rotating or finds an optimal spo t where it is fitted on data?  \\nA place where the highest RSquared value is found, is the place where the line comes to rest.  RSquared represents the amount of variance captured by the virtual linear regression line with  respect to the total variance captured by the dataset.  ',\n",
       " '73 How does the SVM algorithm deal with self-learning?  \\nSVM has a learning rate and expansion rate which takes care of this. The learning rate  compensates or penalises the hyperplanes for making all the wrong moves and expansion rate  deals with finding the maximum separation area between classes.  ',\n",
       " '74 How do you handle outliers in the data?  \\nOutlier is an observation in the data set that is far away from other observations in the data set.  We can discover outliers using tools and functions like box plot, scatter plot, Z-Score, IQR score etc. and then handle them based on the visualization we have got. To handle outliers, we can  cap at some threshold, use transformations to reduce skewness of the data and remove outliers  if they are anomalies or errors.  ',\n",
       " '75 Name and define techniques used to find similarities in the recommendation system.  Pearson correlation and Cosine correlation are techniques used to find similarities in  recommendation systems.  ',\n",
       " '76 Why would you Prune your tree?  \\nIn the context of data science or AIML, pruning refers to the process of reducing redundant  branches of a decision tree. Decision Trees are prone to overfitting, pruning the tree helps to  reduce the size and minimizes the chances of overfitting. Pruning involves turning branches of a  decision tree into leaf nodes and removing the leaf nodes from the original branch. It serves as  a tool to perform the tradeoff.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '77 Mention some of the EDA Techniques?  \\nExploratory Data Analysis (EDA) helps analysts to understand the data better and forms the  foundation of better models.  \\nVisualization  \\n● Univariate visualization  \\n● Bivariate visualization  \\n● Multivariate visualization  \\nMissing Value Treatment – Replace missing values with Either Mean/Median  Outlier Detection – Use Boxplot to identify the distribution of Outliers, then Apply IQR to set the boundary for IQR  ',\n",
       " '78 What is data augmentation? Can you give some examples? Data augmentation is a technique for synthesizing new data by modifying existing data in such a  way that the target is not changed, or it is changed in a known way.  \\nCV is one of the fields where data augmentation is very useful. There are many modifications  that we can do to images:  \\n● Resize  \\n● Horizontal or vertical flip  \\n● Rotate  \\n● Add noise  \\n● Deform  \\n● Modify colors  \\nEach problem needs a customized data augmentation pipeline. For example, on OCR, doing  flips will change the text and won’t be beneficial; however, resizes and small rotations may help.  ',\n",
       " '79 What is Inductive Logic Programming in Machine Learning (ILP)?  Inductive Logic Programming (ILP) is a subfield of machine learning which uses logic  programming representing background knowledge and examples.  ',\n",
       " '80 What is the difference between inductive machine learning and deductive machin e learning?  \\nThe difference between inductive machine learning and deductive machine learning are as  follows: machine-learning where the model learns by examples from a set of observed  instances to draw a generalized conclusion whereas in deductive learning the model first draws  the conclusion and then the conclusion is drawn.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '81 Difference between machine learning and deep learning  \\nMachine learning is a branch of computer science and a method to implement artificial  intelligence. This technique provides the ability to automatically learn and improve from  experiences without being explicitly programmed.  \\nDeep learning can be said as a subset of machine learning. It is mainly based on the artificial  neural network where data is taken as an input and the technique makes intuitive decisions  using the artificial neural network.  ',\n",
       " '82 What Are The Steps Involved In Machine Learning Project?  As you plan for doing a machine learning project. There are several important steps you must  follow to achieve a good working model and they are data collection, data preparation, choosing  a machine learning model, training the model, model evaluation, parameter tuning and lastly  prediction.  ',\n",
       " '83 Differences between Artificial Intelligence and Machine Learning?  Artificial intelligence is a broader prospect than machine learning. Artificial intelligence mimics the cognitive functions of the human brain. The purpose of AI is to carry out a task in an  intelligent manner based on algorithms. On the other hand, machine learning is a subclass of  artificial intelligence. To develop an autonomous machine in such a way so that it can learn  without being explicitly programmed is the goal of machine learning.  ',\n",
       " '84 Steps Needed to Choose the Appropriate Machine Learning Algorithm for  your Classification problem.  \\nFirstly, you need to have a clear picture of your data, your constraints, and your problems  before heading towards different machine learning algorithms. Secondly, you have to  understand which type and kind of data you have because it plays a primary role in deciding  which algorithm you have to use.  \\nFollowing this step is the data categorization step, which is a two-step process – categorizati on by input and categorization by output. The next step is to understand your constraints; that is,  what is your data storage capacity? How fast the prediction has to be? etc.  \\nFinally, find the available machine learning algorithms and implement them wisely. Along with  that, also try to optimize the hyperparameters which can be done in three ways – grid search,  random search, and Bayesian optimization.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '85 Explain Backpropagation in Machine Learning.  \\nA very important question for your machine learning interview. Backpropagation is the  algorithm for computing artificial neural networks (ANN). It is used by the gradient descent  optimization that exploits the chain rule. By calculating the gradient of the loss function, the  weight of the neurons is adjusted to a certain value. To train a multi-layered neural network is  the prime motivation of backpropagation so that it can learn the appropriate internal  demonstrations. This will help them learn to map any input to its respective output arbitrarily.  ',\n",
       " '86 What is the Convex Function?  \\nThis question is very often asked in machine learning interviews. A convex function is a  continuous function, and the value of the midpoint at every interval in its given domain is less  than the numerical mean of the values at the two ends of the interval.  ',\n",
       " '87 What’s the Relationship between True Positive Rate and Recall?  The True positive rate in machine learning is the percentage of the positives that have been  properly acknowledged, and recall is just the count of the results that have been correctly  identified and are relevant. Therefore, they are the same things, just having different names. It is  also known as sensitivity.  ',\n",
       " '88 List some Tools for Parallelizing Machine Learning Algorithms.  Although this question may seem very easy, make sure not to skip this one because it is also  very closely related to artificial intelligence and thereby, AI interview questions. Almos t all machine learning algorithms are easy to serialize. Some of the basic tools for parallelizing are  Matlab, Weka, R, Octave, or the Python-based sci-kit learn. ',\n",
       " '89 What do you mean by Genetic Programming?  \\nGenetic Programming (GP) is almost similar to an Evolutionary Algorithm, a subset of mac hine learning. Genetic programming software systems implement an algorithm that uses random  mutation, a fitness function, crossover, and multiple generations of evolution to resolve a  user-defined task. The genetic programming model is based on testing and choosing the best  option among a set of results.  ',\n",
       " \"90 What do you know about Bayesian Networks?  \\nBayesian Networks also referred to as 'belief networks' or 'casual networks', are used to  represent the graphical model for probability relationship among a set of variables.  For example, a Bayesian network can be used to represent the probabilistic relationships  between diseases and symptoms. As per the symptoms, the network can also compute the  probabilities of the presence of various diseases.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/\\nEfficient algorithms can perform inference or learning in Bayesian networks. Bayesian networks  which relate the variables (e.g., speech signals or protein sequences) are called dynamic  Bayesian networks.  \",\n",
       " '91 Which are the two components of the Bayesian logic program?  A Bayesian logic program consists of two components:  \\n● Logical It contains a set of Bayesian Clauses, which capture the qualitative structure of the domain.  \\n● Quantitative It is used to encode quantitative information about the domain.  ',\n",
       " '92 How is machine learning used in day-to-day life?  \\nMost of the people are already using machine learning in their everyday life. Assume that you  are engaging with the internet, you are actually expressing your preferences, likes, dislikes  through your searches. All these things are picked up by cookies coming on your computer,  from this, the behavior of a user is evaluated. It helps to increase the progress of a user through  the internet and provide similar suggestions.  \\nThe navigation system can also be considered as one of the examples where we are using  machine learning to calculate a distance between two places using optimization techniques.  Surely, people are going to more engage with machine learning in the near future  ',\n",
       " '93 Define Sampling. Why do we need it?  \\nAnswer: Sampling is a process of choosing a subset from a target population that would serve  as its representative. We use the data from the sample to understand the pattern in the  community as a whole. Sampling is necessary because often, we can not gather or process the  complete data within a reasonable time.  ',\n",
       " '94 What does the term decision boundary mean?  \\nAnswer: A decision boundary or a decision surface is a hypersurface which divides the  underlying feature space into two subspaces, one for each class. If the decision boundary is a  hyperplane, then the classes are linearly separable.  ',\n",
       " '95 Define entropy?  \\nAnswer: Entropy is the measure of uncertainty associated with random variable Y. It is the  expected number of bits required to communicate the value of the variable.  ',\n",
       " '96 Indicate the top intents of machine learning?  \\nAnswer: The top intents of machine learning are stated below,  \\n● The system gets information from the already established computations to give  well-founded decisions and outputs.  \\n● It locates certain patterns in the data and then makes certain predictions on it to provide  answers on matters.  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/',\n",
       " '97 Highlight the differences between the Generative model and the  Discriminative model?  \\nThe aim of the Generative model is to generate new samples from the same distribution and  new data instances, Whereas, the Discriminative model highlights the differences between  different kinds of data instances. It tries to learn directly from the data and then classifi es the data.  ',\n",
       " '98 Identify the most important aptitudes of a machine learning engineer?  Machine learning allows the computer to learn itself without being decidedly programmed. It  helps the system to learn from experience and then improve from its mistakes. The intelligenc e system, which is based on machine learning, can learn from recorded data and past incidents.  In-depth knowledge of statistics, probability, data modelling, programming language, as well as  CS, Application of ML Libraries and algorithms, and software design is required to become a  successful machine learning engineer.  ',\n",
       " '99 What is feature engineering? How do you apply it in the process of  modelling?  \\nFeature engineering is the process of transforming raw data into features that better represent  the underlying problem to the predictive models, resulting in improved model accuracy on  unseen data.  ',\n",
       " '100 How can learning curves help create a better model?  \\nLearning curves give the indication of the presence of overfitting or underfitting.  In a learning curve, the training error and cross-validating error are plotted against the number  of training data points.  \\nReferences  \\n1 springboard.com 2 simplilearn.com 3 geeksforgeeks.org 4 elitedatascience.com 5 analyticsvidhya.com 6 guru99.com 7 intellipaat.com 8 towardsdatascience.com 9 mygreatlearning.com 10 mindmajix.com 11 toptal.com 12 glassdoor.co.in 13 udacity.com 14 educba.com 15 analyticsindiamag.com 16 ubuntupit.com 17 javatpoint.com 18 quora.com 19 hackr.io 20 kaggle.com  \\nSteve Nouri https://www.linkedin.com/in/stevenouri/\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preguntas = []\n",
    "for texto in textos:\n",
    "    texto = texto[:texto.find('?')]  \n",
    "\n",
    "for texto in textos:\n",
    "    texto = texto[:texto.find('.')]     \n",
    "textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6351fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q1 Explain the difference between supervised and unsupervised machine  learning',\n",
       " '2 What are the parametric models',\n",
       " '3 What is the difference between classification and regression',\n",
       " '4 What Is Overfitting, and How Can You Avoid It',\n",
       " '5 What is meant by ‘Training set’ and ‘Test Set’',\n",
       " '6 How Do You Handle Missing or Corrupted Data in a Dataset',\n",
       " '7 Explain Ensemble learning. \\nIn ensemble learning, many base models like classifiers and regressors are generated and  combined together so that they give better results. It is used when we build component  classifiers that are accurate and independent. There are sequential as well as parallel ensemble  methods. ',\n",
       " \"8 Explain the Bias-Variance Tradeoff.  \\nPredictive models have a tradeoff between bias (how well the model fits the data) and variance  (how much the model changes based on changes in the inputs).  \\nSimpler models are stable (low variance) but they don't get close to the truth (high bias).  More complex models are more prone to overfitting (high variance) but they are expressive  enough to get close to the truth (low bias). The best model for a given problem usually lies  somewhere in the middle. \",\n",
       " '9 What is the difference between stochastic gradient descent (SGD) and  gradient descent (GD)',\n",
       " '10 How Can You Choose a Classifier Based on a Training Set Data Size',\n",
       " '11 What are 3 data preprocessing techniques to handle outliers',\n",
       " '12 How much data should you allocate for your training, validation, and test  sets',\n",
       " '13 What Is a False Positive and False Negative and How Are They Significant',\n",
       " '14 Explain the difference between L1 and L2 regularization. \\nL2 regularization tends to spread error among all the terms, while L1 is more binary/sparse, with  many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a  Laplacean prior to the terms, while L2 corresponds to a Gaussian prior. ',\n",
       " '15 What’s a Fourier transform',\n",
       " '16 What is deep learning, and how does it contrast with other machine learning  algorithms',\n",
       " '17 What’s the difference between a generative and discriminative model',\n",
       " '18 What Are the Applications of Supervised Machine Learning in Modern  Businesses',\n",
       " '19 What Is Semi-supervised Machine Learning',\n",
       " '20. What Are Unsupervised Machine Learning Techniques',\n",
       " '21 What Is ‘naive’ in the Naive Bayes Classifier',\n",
       " '22 Explain Latent Dirichlet Allocation (LDA).  \\nLatent Dirichlet Allocation (LDA) is a common method of topic modeling, or classifying  documents by subject matter.  \\nLDA is a generative model that represents documents as a mixture of topics that each have  their own probability distribution of possible words.  \\nThe \"Dirichlet\" distribution is simply a distribution of distributions. In LDA, document s are distributions of topics that are distributions of words. ',\n",
       " '23 Explain Principle Component Analysis (PCA).  \\nPCA is a method for transforming features in a dataset by combining them into uncorrelated  linear combinations.  \\nThese new features, or principal components, sequentially maximize the variance represented  (i.e. the first principal component has the most variance, the second principal component has  the second most, and so on).  \\nAs a result, PCA is useful for dimensionality reduction because you can set an arbitrary  variance cutoff. ',\n",
       " '24 What’s the F1 score',\n",
       " '25 When should you use classification over regression',\n",
       " '26 How do you ensure you’re not overfitting with a model',\n",
       " '27 How Will You Know Which Machine Learning Algorithm to Choose for Your  Classification Problem',\n",
       " '28 How Do You Design an Email Spam Filter',\n",
       " '29 What evaluation approaches would you work to gauge the effectiveness of a  machine learning model',\n",
       " '30 How would you implement a recommendation system for our company’s  users',\n",
       " '31 Explain bagging.  \\nBagging, or Bootstrap Aggregating, is an ensemble method in which the dataset is first divided  into multiple subsets through resampling.  \\nThen, each subset is used to train a model, and the final predictions are made through voting or  averaging the component models.  \\nBagging is performed in parallel. ',\n",
       " '32 What is the ROC Curve and what is AUC (a.k.a. AUROC)',\n",
       " '33 Why is Area Under ROC Curve (AUROC) better than raw accuracy as an  out-of-sample evaluation metric',\n",
       " '34 What are the advantages and disadvantages of neural networks',\n",
       " '35 Define Precision and Recall.  \\nPrecision  \\n● Precision is the ratio of several events you can correctly recall to the total number of  events you recall (mix of correct and wrong recalls).  \\n● Precision = (True Positive) / (True Positive + False Positive)  \\nRecall  \\n● A recall is the ratio of a number of events you can recall the number of total events.  ● Recall = (True Positive) / (True Positive + False Negative) ',\n",
       " '36 What Is Decision Tree Classification',\n",
       " '37 What Is Pruning in Decision Trees, and How Is It Done',\n",
       " '38 What Is a Recommendation System',\n",
       " '39 What Is Kernel SVM',\n",
       " '40 What Are Some Methods of Reducing Dimensionality',\n",
       " '41 What Are the Three Stages of Building a Model in Machine Learning',\n",
       " '42 How is KNN different from k-means clustering',\n",
       " '43 Mention the difference between Data Mining and Machine learning',\n",
       " '44 What are the different Algorithm techniques in Machine Learning',\n",
       " '45 You are given a data set. The data set has missing values that spread along 1  standard deviation from the median. What percentage of data would remain  unaffected',\n",
       " '46 What are PCA, KPCA, and ICA used for',\n",
       " '47 What are support vector machines',\n",
       " '48 What is batch statistical learning',\n",
       " '49 What is the bias-variance decomposition of classification error in the  ensemble method',\n",
       " '50 When is Ridge regression favorable over Lasso regression',\n",
       " '51 You’ve built a random forest model with 10000 trees. You got delighted after  getting training error as 0.00. But, the validation error is 34.23. What is going on',\n",
       " '50 What is a convex hull',\n",
       " '51 What do you understand by Type I vs Type II error',\n",
       " '52. In k-means or kNN, we use euclidean distance to calculate the distance  between nearest neighbors. Why not manhattan distance',\n",
       " '53 Do you suggest that treating a categorical variable as a continuous variable  would result in a better predictive model',\n",
       " '54 OLS is to linear regression. The maximum likelihood is logistic regression.  Explain the statement.  \\nOLS and Maximum likelihood are the methods used by the respective regression methods to  approximate the unknown parameter (coefficient) value. In simple words,  Ordinary least square(OLS) is a method used in linear regression which approximates the  parameters resulting in minimum distance between actual and predicted values. Maximum  Likelihood helps in choosing the values of parameters which maximizes the likelihood that the  parameters are most likely to produce observed data. ',\n",
       " '55 When does regularization becomes necessary in Machine Learning',\n",
       " '56 What is Linear Regression',\n",
       " '57 What is the Variance Inflation Factor',\n",
       " '58 We know that one hot encoding increases the dimensionality of a dataset,  but label encoding doesn’t. How',\n",
       " '59 What is a Decision Tree',\n",
       " '60 What is the Binarizing of data',\n",
       " '61 What is cross-validation',\n",
       " '62 When would you use random forests Vs SVM and why',\n",
       " '63 What are the drawbacks of a linear model',\n",
       " '64 Do you think 50 small decision trees are better than a large one',\n",
       " '65 What is a kernel',\n",
       " '66 State the differences between causality and correlation',\n",
       " '67 What is the exploding gradient problem while using the backpropagation  technique',\n",
       " '68 What do you mean by Associative Rule Mining (ARM)',\n",
       " '69 What is Marginalisation',\n",
       " '70 Why is the rotation of components so important in Principle Component  Analysis(PCA)',\n",
       " '71 What is the difference between regularization and normalisation',\n",
       " '72 When does the linear regression line stop rotating or finds an optimal spo t where it is fitted on data',\n",
       " '73 How does the SVM algorithm deal with self-learning',\n",
       " '74 How do you handle outliers in the data',\n",
       " '75 Name and define techniques used to find similarities in the recommendation system.  Pearson correlation and Cosine correlation are techniques used to find similarities in  recommendation systems. ',\n",
       " '76 Why would you Prune your tree',\n",
       " '77 Mention some of the EDA Techniques',\n",
       " '78 What is data augmentation',\n",
       " '79 What is Inductive Logic Programming in Machine Learning (ILP)',\n",
       " '80 What is the difference between inductive machine learning and deductive machin e learning',\n",
       " '81 Difference between machine learning and deep learning  \\nMachine learning is a branch of computer science and a method to implement artificial  intelligence. This technique provides the ability to automatically learn and improve from  experiences without being explicitly programmed.  \\nDeep learning can be said as a subset of machine learning. It is mainly based on the artificial  neural network where data is taken as an input and the technique makes intuitive decisions  using the artificial neural network. ',\n",
       " '82 What Are The Steps Involved In Machine Learning Project',\n",
       " '83 Differences between Artificial Intelligence and Machine Learning',\n",
       " '84 Steps Needed to Choose the Appropriate Machine Learning Algorithm for  your Classification problem.  \\nFirstly, you need to have a clear picture of your data, your constraints, and your problems  before heading towards different machine learning algorithms. Secondly, you have to  understand which type and kind of data you have because it plays a primary role in deciding  which algorithm you have to use.  \\nFollowing this step is the data categorization step, which is a two-step process – categorizati on by input and categorization by output. The next step is to understand your constraints; that is,  what is your data storage capacity',\n",
       " '85 Explain Backpropagation in Machine Learning.  \\nA very important question for your machine learning interview. Backpropagation is the  algorithm for computing artificial neural networks (ANN). It is used by the gradient descent  optimization that exploits the chain rule. By calculating the gradient of the loss function, the  weight of the neurons is adjusted to a certain value. To train a multi-layered neural network is  the prime motivation of backpropagation so that it can learn the appropriate internal  demonstrations. This will help them learn to map any input to its respective output arbitrarily. ',\n",
       " '86 What is the Convex Function',\n",
       " '87 What’s the Relationship between True Positive Rate and Recall',\n",
       " '88 List some Tools for Parallelizing Machine Learning Algorithms.  Although this question may seem very easy, make sure not to skip this one because it is also  very closely related to artificial intelligence and thereby, AI interview questions. Almos t all machine learning algorithms are easy to serialize. Some of the basic tools for parallelizing are  Matlab, Weka, R, Octave, or the Python-based sci-kit learn.',\n",
       " '89 What do you mean by Genetic Programming',\n",
       " '90 What do you know about Bayesian Networks',\n",
       " '91 Which are the two components of the Bayesian logic program',\n",
       " '92 How is machine learning used in day-to-day life',\n",
       " '93 Define Sampling. Why do we need it',\n",
       " '94 What does the term decision boundary mean',\n",
       " '95 Define entropy',\n",
       " '96 Indicate the top intents of machine learning',\n",
       " '97 Highlight the differences between the Generative model and the  Discriminative model',\n",
       " '98 Identify the most important aptitudes of a machine learning engineer',\n",
       " '99 What is feature engineering',\n",
       " '100 How can learning curves help create a better model']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preguntas = []\n",
    "for texto in textos:\n",
    "    texto = preguntas.append(texto[:texto.find('?')])    \n",
    "\n",
    "    if texto.find('.') > 0:\n",
    "        texto = preguntas.append(texto[:texto.find('.')])     \n",
    "preguntas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e242e6b8c453217117083c6680e6db82176874e34559806b32fd7417ddae620"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
